# Code Review: MS Graph AI Agent Model

**Review Date:** 2025-12-20
**Reviewer:** Claude Code
**Repository:** ms-graph-ai-agent-model

---

## Executive Summary

This project implements a fine-tuned LLM for Microsoft Graph API tool calling. The overall architecture is sound and the approach is clever, but there are several critical issues that need attention, particularly around error handling, dependency management, and documentation consistency.

**Overall Grade:** C+ (Functional but needs significant improvements)

---

## Critical Issues ðŸ”´

### 1. No Dependency Version Pinning (`requirements.txt`)
**Severity:** CRITICAL
**Location:** `requirements.txt:1-10`

```txt
torch
transformers
peft
```

**Issue:** No versions specified for any dependencies. This will cause:
- Non-reproducible builds across different systems
- Breaking changes when packages update
- Difficult debugging when issues arise

**Recommendation:**
```txt
torch==2.5.1
transformers==4.46.0
peft==0.13.2
trl==0.12.1
bitsandbytes==0.44.1
accelerate==1.1.1
datasets==3.1.0
scipy==1.14.1
pyyaml==6.0.2
requests==2.32.3
```

### 2. Bare Exception Handling (`1_graph_api_harvester.py:110`)
**Severity:** CRITICAL
**Location:** `1_graph_api_harvester.py:110`

```python
except: continue
```

**Issue:** Silently swallows ALL exceptions (including KeyboardInterrupt, SystemExit). Makes debugging impossible.

**Recommendation:**
```python
except Exception as e:
    print(f"Warning: Failed to process {method.upper()} {path}: {e}")
    continue
```

### 3. Hardcoded Flash Attention (`2_train_graph_agent_model.py:56`)
**Severity:** HIGH
**Location:** `2_train_graph_agent_model.py:56`

```python
attn_implementation="flash_attention_2"
```

**Issue:** Will crash on GPUs that don't support Flash Attention 2 (requires Ampere or newer). No fallback mechanism.

**Recommendation:**
```python
# Try Flash Attention 2, fallback to SDPA
try:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        attn_implementation="flash_attention_2"
    )
except Exception as e:
    print(f"Flash Attention 2 not available, using SDPA: {e}")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        attn_implementation="sdpa"
    )
```

### 4. Hardcoded CUDA Device (`3_evaluate_model.py:43`, `4_interactive_graph.py:40`)
**Severity:** HIGH
**Locations:**
- `3_evaluate_model.py:43`
- `4_interactive_graph.py:40`

```python
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
```

**Issue:** Crashes on CPU-only systems or when CUDA is unavailable.

**Recommendation:**
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
```

### 5. No Input Validation Before Training (`2_train_graph_agent_model.py:40`)
**Severity:** MEDIUM
**Location:** `2_train_graph_agent_model.py:40`

**Issue:** No check if dataset file exists or is empty before starting expensive training.

**Recommendation:**
```python
if not os.path.exists(DATA_FILE):
    raise FileNotFoundError(f"Dataset not found at {DATA_FILE}. Run 1_graph_api_harvester.py first.")

dataset = load_dataset("json", data_files=DATA_FILE, split="train")
if len(dataset) == 0:
    raise ValueError(f"Dataset is empty! Check {DATA_FILE}")
print(f"Loaded {len(dataset)} training examples.")
```

---

## Documentation Mismatches ðŸ“„

### README vs. Code Inconsistencies

| README Reference | Actual Code | Location |
|-----------------|-------------|----------|
| Output: `./data_graph/` | `./data/` | `1_graph_api_harvester.py:9` |
| Script: `2_train_graph_agent.py` | `2_train_graph_agent_model.py` | Filename |
| Script: `3_evaluate_agent.py` | `3_evaluate_model.py` | Filename |
| Model output: `./graph-sentinel-v1` | `./ms-graph-v1` | `2_train_graph_agent_model.py:10` |
| LoRA rank: 16 | 32 | `2_train_graph_agent_model.py:69` |

**Impact:** Users following the README will encounter errors.

**Recommendation:** Update README or code to match (preferably update code to match README for consistency).

---

## Code Quality Issues âš ï¸

### 1. Missing Type Hints
**Severity:** MEDIUM
**All Files**

**Issue:** No type hints anywhere. Reduces IDE support and makes code harder to maintain.

**Example Fix:**
```python
def clean_text(text: str | None) -> str:
    if not text:
        return ""
    text = re.sub('<[^<]+?>', '', text)
    return re.sub(r'\s+', ' ', text).strip()

def format_tool(path: str, method: str, op: dict) -> dict:
    # ...
```

### 2. Missing Docstrings
**Severity:** MEDIUM
**All Files**

**Issue:** Only `get_properties_recursive` and `format_chat_template` have docstrings. All public functions should be documented.

**Example:**
```python
def download_spec() -> dict:
    """
    Downloads the Microsoft Graph OpenAPI specification from GitHub.

    Returns:
        dict: Parsed YAML specification as a dictionary

    Raises:
        Exception: If download fails or returns non-200 status
    """
```

### 3. No Logging Framework
**Severity:** LOW
**All Files**

**Issue:** Uses `print()` statements instead of proper logging. Makes it hard to control verbosity.

**Recommendation:**
```python
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

logger.info("Downloading OpenAPI Spec...")
logger.warning(f"Failed to process {path}: {e}")
```

### 4. Naive Tool Routing (`4_interactive_graph.py:56`)
**Severity:** MEDIUM
**Location:** `4_interactive_graph.py:56`

```python
tool_key = "mail" if "mail" in user_input or "send" in user_input else "calendar"
```

**Issue:** Simple keyword matching won't work for:
- "Can you check my schedule?" â†’ Should be calendar, but will default to calendar anyway
- "I need to send a calendar invite" â†’ Will route to mail incorrectly

**Recommendation:** Add a comment noting this is for demo only, or implement a proper classifier.

### 5. Unused Dependency
**Severity:** LOW
**Location:** `requirements.txt:8`

**Issue:** `scipy` is listed but never imported or used in any script.

**Recommendation:** Remove from requirements unless needed by transitive dependencies.

### 6. Magic Values Not Extracted
**Severity:** LOW
**Multiple Locations**

**Examples:**
- Temperature: `0.1` (appears in 2 files)
- Max tokens: `256` (appears in 2 files)
- Context window: `2048`

**Recommendation:** Extract to configuration constants.

---

## Security Concerns ðŸ”’

### 1. No Input Sanitization (`4_interactive_graph.py`)
**Severity:** MEDIUM
**Location:** `4_interactive_graph.py:52-58`

**Issue:** User input is passed directly to the model without validation. While not immediately exploitable, this could lead to prompt injection attacks in production.

**Recommendation:**
```python
def sanitize_input(user_input: str) -> str:
    """Remove potentially dangerous characters from user input."""
    # Limit length
    if len(user_input) > 500:
        raise ValueError("Input too long (max 500 chars)")
    # Remove special tokens if using for production
    dangerous_patterns = ["<|im_start|>", "<|im_end|>", "<|endoftext|>"]
    for pattern in dangerous_patterns:
        if pattern in user_input:
            raise ValueError(f"Input contains forbidden pattern: {pattern}")
    return user_input.strip()
```

### 2. No HTTPS Enforcement
**Severity:** LOW
**Location:** `1_graph_api_harvester.py:11`

**Issue:** URL is hardcoded to HTTP (though GitHub will redirect to HTTPS).

**Recommendation:** Use `https://` explicitly in the URL.

---

## Data Quality Issues ðŸ“Š

### 1. Inconsistent Parameter Type Handling (`1_graph_api_harvester.py:78-86`)
**Severity:** MEDIUM
**Location:** `1_graph_api_harvester.py:78-86`

```python
def generate_dummy_args(params):
    args = {}
    for k, v in params['properties'].items():
        if k == "$filter": args[k] = "startswith(displayName, 'A')"
        elif k == "$select": args[k] = "id,displayName"
        elif v['type'] == 'integer': args[k] = 10
        elif v['type'] == 'boolean': args[k] = True
        else: args[k] = f"example_{k}"
    return args
```

**Issues:**
- Arrays are treated as strings (`$select` should be an array per schema)
- Object types get string placeholders
- No handling for nested objects

**Recommendation:**
```python
def generate_dummy_args(params):
    args = {}
    for k, v in params['properties'].items():
        param_type = v.get('type', 'string')

        # Special handling for OData parameters
        if k == "$filter":
            args[k] = "startswith(displayName, 'A')"
        elif k == "$select":
            args[k] = ["id", "displayName"]  # Array, not string
        elif param_type == 'integer':
            args[k] = 10
        elif param_type == 'boolean':
            args[k] = True
        elif param_type == 'array':
            args[k] = [f"example_{k}_item"]
        elif param_type == 'object':
            args[k] = {}
        else:
            args[k] = f"example_{k}"
    return args
```

### 2. Training Data Shows Type Mismatch
**Severity:** MEDIUM
**Location:** Generated dataset

**Issue:** Looking at the sample data:
```json
{"$select": {"type": "array", ...}}
// But output is:
{"$select": "id,displayName"}  // String, not array!
```

This trains the model on incorrect schema adherence.

---

## Missing Features ðŸš§

### 1. No CLI Argument Support
**All Scripts**

**Issue:** Everything is hardcoded. Users can't easily change:
- Model paths
- Output directories
- Hyperparameters
- Device selection

**Recommendation:** Use `argparse`:
```python
import argparse

parser = argparse.ArgumentParser(description="Train MS Graph Agent")
parser.add_argument("--model-id", default="Qwen/Qwen2.5-7B-Instruct")
parser.add_argument("--output-dir", default="./results")
parser.add_argument("--data-file", default="./data/graph_tool_dataset.jsonl")
parser.add_argument("--epochs", type=int, default=3)
args = parser.parse_args()
```

### 2. No Unit Tests
**Project-wide**

**Issue:** No test suite. Changes could break functionality silently.

**Recommendation:** Add basic tests:
```python
# tests/test_harvester.py
def test_clean_text():
    assert clean_text("<p>Hello</p>") == "Hello"
    assert clean_text("Multiple   spaces") == "Multiple spaces"
    assert clean_text(None) == ""

def test_generate_dummy_args():
    params = {"properties": {"count": {"type": "integer"}}}
    result = generate_dummy_args(params)
    assert result["count"] == 10
```

### 3. No Validation of Model Outputs
**`3_evaluate_model.py`, `4_interactive_graph.py`**

**Issue:** Model output is printed but not validated as valid JSON.

**Recommendation:**
```python
response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
try:
    parsed = json.loads(response)
    print(f"\nâœ“ Valid JSON Output: {json.dumps(parsed, indent=2)}\n")
except json.JSONDecodeError as e:
    print(f"\nâœ— Invalid JSON Output: {e}\nRaw: {response}\n")
```

---

## Positive Aspects âœ…

### What's Done Well

1. **Clear Separation of Concerns** - Each script has a single, well-defined purpose
2. **Schema-First Approach** - Using the official OpenAPI spec is smart
3. **Modern Stack** - QLoRA, Qwen 2.5, proper quantization
4. **Readable Code** - Despite lack of docs, the code flow is clear
5. **Practical Prompts** - The synthetic prompt templates are reasonable
6. **Good LoRA Configuration** - Targeting all linear layers is appropriate
7. **Memory Optimization** - Gradient checkpointing, 4-bit quantization, etc.
8. **Comprehensive README** - Good documentation (despite mismatches)

---

## Recommendations Summary

### Immediate Actions (Before Next Use)

1. âœ… Pin all dependency versions in `requirements.txt`
2. âœ… Fix bare `except` clause in harvester
3. âœ… Add Flash Attention fallback in training script
4. âœ… Add device detection (`cuda` vs `cpu`)
5. âœ… Fix README documentation mismatches
6. âœ… Validate dataset exists before training

### Short-term Improvements (Next Sprint)

1. Add type hints to all functions
2. Add docstrings to all public functions
3. Replace `print` with `logging`
4. Add CLI argument parsing
5. Fix array type handling in `generate_dummy_args`
6. Add JSON validation to evaluation scripts

### Long-term Enhancements (Future)

1. Implement proper test suite
2. Add continuous integration (CI)
3. Implement better tool routing (embeddings/RAG)
4. Add model performance metrics tracking
5. Create configuration file support (YAML/JSON)
6. Add progress bars for long operations

---

## Conclusion

This is a solid proof-of-concept with a clever approach to fine-tuning for tool calling. The core architecture is sound, but production readiness requires addressing:

- **Error handling** (critical for reliability)
- **Dependency management** (critical for reproducibility)
- **Documentation consistency** (critical for usability)

With these fixes, this could be a strong foundation for an enterprise-grade Microsoft Graph AI agent.

**Estimated effort to production-ready:** 2-3 weeks of hardening and testing.
